# ğŸ¬ Scraper AllocinÃ©

Service de scraping automatique pour rÃ©cupÃ©rer les prochaines sorties cinÃ©ma depuis AllocinÃ© et les sauvegarder en base MongoDB.

## ğŸš€ FonctionnalitÃ©s

- **Scraping intelligent** : Parcourt les pages d'agenda d'AllocinÃ© sur 12 semaines
- **Extraction complÃ¨te** : RÃ©cupÃ¨re titre, affiche, date de sortie, genre, rÃ©alisateur, acteurs, synopsis
- **Navigation automatique** : Utilise la pagination d'AllocinÃ© pour naviguer entre les semaines
- **Sauvegarde MongoDB** : Stockage avec dÃ©doublonnage par URL
- **Planification automatique** : ExÃ©cution quotidienne via cron
- **Logs dÃ©taillÃ©s** : Suivi complet avec fichiers de logs
- **Gestion d'erreurs** : Robuste face aux changements de structure HTML

## ğŸ“ Structure du projet

```
scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â””â”€â”€ allocineScraper.js     # Scraper principal
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ mongodb.js             # Gestion MongoDB
â”‚   â”œâ”€â”€ schedulers/
â”‚   â”‚   â””â”€â”€ cronScheduler.js       # Planification cron
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.js              # SystÃ¨me de logs
â”‚       â””â”€â”€ helpers.js             # Utilitaires
â”œâ”€â”€ test/
â”‚   â””â”€â”€ testScraper.js             # Tests
â”œâ”€â”€ logs/                          # Fichiers de logs
â”œâ”€â”€ .env                           # Configuration
â”œâ”€â”€ package.json
â”œâ”€â”€ index.js                       # Point d'entrÃ©e
â””â”€â”€ README.md
```

## âš™ï¸ Installation

1. **Installation des dÃ©pendances**
```bash
cd scraper
npm install
```

2. **Configuration**
Copiez `.env.example` vers `.env` et ajustez les paramÃ¨tres :
```bash
cp .env.example .env
```

Variables importantes :
- `MONGODB_URI` : URI de connexion MongoDB
- `SCRAPE_CRON_SCHEDULE` : Planning cron (dÃ©faut: 2h du matin)
- `MAX_WEEKS` : Nombre de semaines Ã  scraper (dÃ©faut: 12)
- `HEADLESS_MODE` : Mode headless du navigateur (dÃ©faut: true)

## ğŸ® Usage

### Commandes disponibles

```bash
# Scraping manuel immÃ©diat
node index.js run

# DÃ©marrage du scheduler (mode daemon)
node index.js schedule

# Test rapide (1 semaine seulement)
node index.js test

# Affichage de l'aide
node index.js help
```

### Test du scraper

Pour tester avant la mise en production :
```bash
npm run test
# ou
node test/testScraper.js
```

## ğŸ—„ï¸ Structure des donnÃ©es

Chaque film est sauvegardÃ© avec la structure suivante :

```javascript
{
  title: "Titre du film",
  url: "https://www.allocine.fr/film/fichefilm_gen_cfilm=123456.html",
  imageUrl: "https://fr.web.img2.acsta.net/.../poster.jpg",
  releaseDate: Date("2025-09-03"),
  genre: "ComÃ©die",
  director: "RÃ©alisateur",
  cast: "Acteur 1, Acteur 2, Acteur 3",
  synopsis: "Synopsis du film...",
  movieId: "123456",
  scrapedAt: Date("2025-01-15T10:30:00Z"),
  lastUpdated: Date("2025-01-15T10:30:00Z"),
  source: "allocine"
}
```

## ğŸ“Š Collections MongoDB

### `allocine_movies`
Stockage des films avec dÃ©doublonnage par URL.

### `scraping_logs`
Logs des opÃ©rations de scraping pour monitoring.

## ğŸ”§ Configuration avancÃ©e

### Planning cron
Le format cron par dÃ©faut `0 2 * * *` lance le scraper Ã  2h du matin chaque jour.

Exemples de configurations :
- `0 2 * * *` : Quotidien Ã  2h
- `0 2 * * 1` : Chaque lundi Ã  2h
- `0 */6 * * *` : Toutes les 6h

### Mode debug
Pour dÃ©bugger le scraper, modifiez `.env` :
```env
HEADLESS_MODE=false
NODE_ENV=development
```

## ğŸ“ Logs

Les logs sont sauvegardÃ©s dans le dossier `logs/` :
- `scraper.log` : Log gÃ©nÃ©ral
- `error.log` : Erreurs uniquement
- `warn.log` : Avertissements

Format JSON pour faciliter l'analyse :
```json
{
  "timestamp": "2025-01-15T10:30:00.000Z",
  "level": "info",
  "message": "Scraping terminÃ©",
  "data": { "totalMovies": 150 }
}
```

## ğŸš¨ Gestion d'erreurs

Le scraper est conÃ§u pour Ãªtre robuste :
- **Retry automatique** sur les erreurs rÃ©seau
- **Skip des films invalides** sans arrÃªter le processus
- **Navigation de fallback** si les boutons ne rÃ©pondent pas
- **Timeout configurable** pour Ã©viter les blocages
- **User-Agent rotation** pour Ã©viter la dÃ©tection

## ğŸ”’ Bonnes pratiques

1. **Respect des serveurs** : DÃ©lais entre les requÃªtes
2. **User-Agent rÃ©aliste** : Simulation d'un navigateur normal
3. **Gestion des cookies** : Maintien de session
4. **Monitoring** : Surveillance des logs d'erreur

## ğŸ³ DÃ©ploiement

### Docker (optionnel)
```dockerfile
FROM node:18-slim
RUN apt-get update && apt-get install -y chromium
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD ["node", "index.js", "schedule"]
```

### Service systemd
```ini
[Unit]
Description=Allocine Scraper
After=network.target

[Service]
Type=simple
User=nodejs
WorkingDirectory=/path/to/scraper
ExecStart=/usr/bin/node index.js schedule
Restart=always

[Install]
WantedBy=multi-user.target
```

## ğŸ“ˆ Monitoring

Pour surveiller le scraper en production :

1. **Logs d'erreur** : `tail -f logs/error.log`
2. **Status MongoDB** : VÃ©rifier les collections
3. **MÃ©triques** : Nombre de films par scraping
4. **Alertes** : Notification si 0 film scrapÃ©

## ğŸ¤ Contribution

Le code est structurÃ© pour faciliter les modifications :
- SÃ©lecteurs HTML centralisÃ©s dans le scraper
- Configuration externalisÃ©e dans `.env`
- Logs dÃ©taillÃ©s pour debug
- Architecture modulaire

## ğŸ“„ Licence

Usage personnel - Respecter les conditions d'utilisation d'AllocinÃ©.